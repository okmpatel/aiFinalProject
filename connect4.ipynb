{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1901488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 2: connect 4 definitions and minimax agent\n",
    "class Connect4:\n",
    "    def __init__(self, rows=6, cols=7, connect=4):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.connect = connect\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1  # 1 = X, -1 = O\n",
    "        self.last_move = None\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board.copy()\n",
    "\n",
    "    def valid_actions(self):\n",
    "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in self.valid_actions():\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # drop piece\n",
    "        for r in range(self.rows - 1, -1, -1):\n",
    "            if self.board[r, action] == 0:\n",
    "                self.board[r, action] = self.current_player\n",
    "                self.last_move = (r, action)\n",
    "                break\n",
    "\n",
    "        winner = check_winner(self.board, self.connect, self.last_move)\n",
    "        if winner is not None:\n",
    "            self.done = True\n",
    "            self.winner = winner\n",
    "            reward = 1 if winner == self.current_player else -1\n",
    "        elif np.all(self.board != 0):\n",
    "            self.done = True\n",
    "            self.winner = 0\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        if not self.done:\n",
    "            self.current_player *= -1\n",
    "\n",
    "        return self.board.copy(), reward, self.done, {}\n",
    "\n",
    "    def render(self):\n",
    "        sym = {1: 'X', -1: 'O', 0: '.'}\n",
    "        for r in range(self.rows):\n",
    "            print(\" \".join(sym[int(x)] for x in self.board[r]))\n",
    "        print(\" \".join(str(i) for i in range(self.cols)))\n",
    "        print()\n",
    "\n",
    "def check_winner(board, connect=4, last_move=None):\n",
    "    rows, cols = board.shape\n",
    "\n",
    "    if last_move is not None:\n",
    "        r0, c0 = last_move\n",
    "        player = board[r0, c0]\n",
    "        if player == 0:\n",
    "            return None\n",
    "\n",
    "        dirs = [(1,0), (0,1), (1,1), (-1,1)]\n",
    "        for dr, dc in dirs:\n",
    "            count = 1\n",
    "\n",
    "            rr, cc = r0 + dr, c0 + dc\n",
    "            while 0 <= rr < rows and 0 <= cc < cols and board[rr,cc] == player:\n",
    "                count += 1\n",
    "                rr += dr; cc += dc\n",
    "\n",
    "            rr, cc = r0 - dr, c0 - dc\n",
    "            while 0 <= rr < rows and 0 <= cc < cols and board[rr,cc] == player:\n",
    "                count += 1\n",
    "                rr -= dr; cc -= dc\n",
    "\n",
    "            if count >= connect:\n",
    "                return player\n",
    "\n",
    "        return None\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if board[r,c] == 0:\n",
    "                continue\n",
    "            player = board[r,c]\n",
    "            for dr, dc in [(1,0),(0,1),(1,1),(-1,1)]:\n",
    "                cnt = 0\n",
    "                rr, cc = r, c\n",
    "                while 0 <= rr < rows and 0 <= cc < cols and board[rr,cc] == player:\n",
    "                    cnt += 1\n",
    "                    rr += dr; cc += dc\n",
    "                if cnt >= connect:\n",
    "                    return player\n",
    "    return None\n",
    "\n",
    "def heuristic_evaluate(board, player):\n",
    "    rows, cols = board.shape\n",
    "    opponent = -player\n",
    "\n",
    "    SCORE = {4: 100000, 3: 100, 2: 10}\n",
    "    total = 0\n",
    "\n",
    "    def score_window(window):\n",
    "        s = 0\n",
    "        cnt_p = np.count_nonzero(window == player)\n",
    "        cnt_o = np.count_nonzero(window == opponent)\n",
    "        cnt_e = np.count_nonzero(window == 0)\n",
    "\n",
    "        if cnt_p == 4:\n",
    "            s += SCORE[4]\n",
    "        elif cnt_p == 3 and cnt_e == 1:\n",
    "            s += SCORE[3]\n",
    "        elif cnt_p == 2 and cnt_e == 2:\n",
    "            s += SCORE[2]\n",
    "\n",
    "        if cnt_o == 4:\n",
    "            s -= SCORE[4]\n",
    "        elif cnt_o == 3 and cnt_e == 1:\n",
    "            s -= SCORE[3]*0.9\n",
    "        elif cnt_o == 2 and cnt_e == 2:\n",
    "            s -= SCORE[2]*0.5\n",
    "\n",
    "        return s\n",
    "\n",
    "    # horizontal\n",
    "    for r in range(rows):\n",
    "        for c in range(cols-3):\n",
    "            window = board[r, c:c+4]\n",
    "            total += score_window(window)\n",
    "\n",
    "    # vertical\n",
    "    for c in range(cols):\n",
    "        for r in range(rows-3):\n",
    "            window = board[r:r+4, c]\n",
    "            total += score_window(window)\n",
    "\n",
    "    # diag down-right\n",
    "    for r in range(rows-3):\n",
    "        for c in range(cols-3):\n",
    "            window = np.array([board[r+i, c+i] for i in range(4)])\n",
    "            total += score_window(window)\n",
    "\n",
    "    # diag up-right\n",
    "    for r in range(3, rows):\n",
    "        for c in range(cols-3):\n",
    "            window = np.array([board[r-i, c+i] for i in range(4)])\n",
    "            total += score_window(window)\n",
    "\n",
    "    # center column preference\n",
    "    center = cols // 2\n",
    "    total += np.count_nonzero(board[:, center] == player) * 3\n",
    "\n",
    "    return total\n",
    "\n",
    "def minimax_ab(board, depth, alpha, beta, maximizing, player_to_move, env):\n",
    "    w = check_winner(board, env.connect, last_move=None)\n",
    "    if w is not None:\n",
    "        return (1e9 if w == 1 else -1e9), None\n",
    "    if np.all(board != 0):\n",
    "        return 0, None\n",
    "    if depth == 0:\n",
    "        return heuristic_evaluate(board, player_to_move), None\n",
    "\n",
    "    valid_moves = [c for c in range(env.cols) if board[0, c] == 0]\n",
    "    valid_moves.sort(key=lambda c: -abs(c - env.cols//2))  # center-first move ordering\n",
    "\n",
    "    best_move = None\n",
    "\n",
    "    if maximizing:\n",
    "        value = -math.inf\n",
    "        for col in valid_moves:\n",
    "            new_board = board.copy()\n",
    "            for r in range(env.rows-1, -1, -1):\n",
    "                if new_board[r, col] == 0:\n",
    "                    new_board[r, col] = player_to_move\n",
    "                    last_r = r\n",
    "                    break\n",
    "\n",
    "            w = check_winner(new_board, env.connect, last_move=(last_r, col))\n",
    "            if w is not None:\n",
    "                v = 1e9\n",
    "            else:\n",
    "                v, _ = minimax_ab(new_board, depth-1, alpha, beta, False, -player_to_move, env)\n",
    "\n",
    "            if v > value:\n",
    "                value = v\n",
    "                best_move = col\n",
    "\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break\n",
    "\n",
    "        return value, best_move\n",
    "\n",
    "    else:\n",
    "        value = math.inf\n",
    "        for col in valid_moves:\n",
    "            new_board = board.copy()\n",
    "            for r in range(env.rows-1, -1, -1):\n",
    "                if new_board[r, col] == 0:\n",
    "                    new_board[r, col] = player_to_move\n",
    "                    last_r = r\n",
    "                    break\n",
    "\n",
    "            w = check_winner(new_board, env.connect, last_move=(last_r, col))\n",
    "            if w is not None:\n",
    "                v = -1e9\n",
    "            else:\n",
    "                v, _ = minimax_ab(new_board, depth-1, alpha, beta, True, -player_to_move, env)\n",
    "\n",
    "            if v < value:\n",
    "                value = v\n",
    "                best_move = col\n",
    "\n",
    "            beta = min(beta, value)\n",
    "            if alpha >= beta:\n",
    "                break\n",
    "\n",
    "        return value, best_move\n",
    "    \n",
    "def minimax_agent(env, depth=4):\n",
    "    board = env.board.copy()\n",
    "    player = env.current_player\n",
    "    value, action = minimax_ab(board, depth, -math.inf, math.inf, True, player, env)\n",
    "    if action is None:\n",
    "        return random.choice(env.valid_actions())\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3: game loop human vs AI(minimax)\n",
    "env = Connect4()\n",
    "game_over = False\n",
    "\n",
    "print(\"Starting Connect Four\")\n",
    "env.render()\n",
    "\n",
    "# 0 = Human, 1 = AI\n",
    "turn = 0\n",
    "\n",
    "while not game_over:\n",
    "\n",
    "    if turn == 0:\n",
    "        # Human move\n",
    "        valid = env.valid_actions()\n",
    "        col = None\n",
    "        while col not in valid:\n",
    "            try:\n",
    "                col_input = int(input(f\"Your move {valid}: \"))\n",
    "                if col_input in valid:\n",
    "                    col = col_input\n",
    "                else:\n",
    "                    print(\"Invalid column. Choose from\", valid)\n",
    "            except:\n",
    "                print(\"Enter a valid integer column.\")\n",
    "        \n",
    "        _, _, game_over, _ = env.step(col)\n",
    "\n",
    "        if env.winner == 1:\n",
    "            env.render()\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # AI move using minimax\n",
    "        print(\"AI thinking...\")\n",
    "        col = minimax_agent(env, depth=4)\n",
    "        _, _, game_over, _ = env.step(col)\n",
    "\n",
    "        if env.winner == -1:\n",
    "            env.render()\n",
    "            print(\"AI wins!\")\n",
    "            break\n",
    "\n",
    "    env.render()\n",
    "    turn = 1 - turn  # switch turns\n",
    "\n",
    "    # Check draw\n",
    "    if game_over and env.winner == 0:\n",
    "        print(\"Draw!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed750bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "q_table = defaultdict(lambda: np.zeros(7, dtype=float))\n",
    "\n",
    "alpha = 0.25\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay_steps = 50000\n",
    "\n",
    "total_episodes = 50000    # full curriculum length\n",
    "min_episodes = 20000\n",
    "\n",
    "curriculum = [\n",
    "    (\"random\", 0.25),     # 25% vs random\n",
    "    (\"minimax2\", 0.35),   # 35% vs minimax depth 2\n",
    "    (\"selfplay\", 0.40)    # final 40% self-play\n",
    "]\n",
    "\n",
    "\n",
    "def extract_features(board, player):\n",
    "    opp = -player\n",
    "    rows, cols = board.shape\n",
    "\n",
    "    def count_patterns(val, length):\n",
    "        cnt = 0\n",
    "\n",
    "        # horizontal\n",
    "        for r in range(rows):\n",
    "            for c in range(cols - 3):\n",
    "                w = list(board[r, c:c+4])\n",
    "                if w.count(val) == length and w.count(0) == (4 - length):\n",
    "                    cnt += 1\n",
    "\n",
    "        # vertical\n",
    "        for c in range(cols):\n",
    "            for r in range(rows - 3):\n",
    "                w = list(board[r:r+4, c])\n",
    "                if w.count(val) == length and w.count(0) == (4 - length):\n",
    "                    cnt += 1\n",
    "\n",
    "        # diag down-right\n",
    "        for r in range(rows - 3):\n",
    "            for c in range(cols - 3):\n",
    "                w = [board[r+i, c+i] for i in range(4)]\n",
    "                if w.count(val) == length and w.count(0) == (4 - length):\n",
    "                    cnt += 1\n",
    "\n",
    "        # diag up-right\n",
    "        for r in range(3, rows):\n",
    "            for c in range(cols - 3):\n",
    "                w = [board[r-i, c+i] for i in range(4)]\n",
    "                if w.count(val) == length and w.count(0) == (4 - length):\n",
    "                    cnt += 1\n",
    "\n",
    "        return cnt\n",
    "\n",
    "    p2 = min(count_patterns(player, 2), 4)\n",
    "    p3 = min(count_patterns(player, 3), 4)\n",
    "    o2 = min(count_patterns(opp, 2), 4)\n",
    "    o3 = min(count_patterns(opp, 3), 4)\n",
    "\n",
    "    center_col = cols // 2\n",
    "    center_ctrl = int(np.count_nonzero(board[:, center_col] == player))\n",
    "\n",
    "    valid = [1 if board[0, c] == 0 else 0 for c in range(cols)]\n",
    "    mask_thirds = (sum(valid[0:3]) > 0) * 1 + (sum(valid[2:5]) > 0) * 2 + (sum(valid[4:7]) > 0) * 4\n",
    "\n",
    "    return (p2, p3, o2, o3, min(center_ctrl, 6), mask_thirds)\n",
    "\n",
    "def state_key(board, player):\n",
    "    return (extract_features(board, player), player)\n",
    "\n",
    "def shaped_reward(board, player, base_reward):\n",
    "    p2, p3, o2, o3, center, _ = extract_features(board, player)\n",
    "\n",
    "    r = 0\n",
    "    r += 0.08 * p2\n",
    "    r += 0.35 * p3\n",
    "    r -= 0.08 * o2\n",
    "    r -= 0.6 * o3\n",
    "    r += 0.03 * center\n",
    "\n",
    "    return r + base_reward\n",
    "\n",
    "\n",
    "def choose_action(state_key, valid, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        # exploration but biased toward center\n",
    "        if random.random() < 0.7:\n",
    "            center = 3\n",
    "            return sorted(valid, key=lambda c: abs(c - center))[0]\n",
    "        return random.choice(valid)\n",
    "\n",
    "    q_vals = q_table[state_key]\n",
    "    masked = np.full_like(q_vals, -np.inf)\n",
    "    masked[valid] = q_vals[valid]\n",
    "    return int(np.argmax(masked))\n",
    "\n",
    "def evaluate_vs_minimax(depth=2, n_games=50):\n",
    "    wins = losses = draws = 0\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        env = Connect4()\n",
    "        turn = random.choice([1, -1])\n",
    "        env.current_player = turn\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                col = q_agent(env)\n",
    "            else:\n",
    "                col = minimax_agent(env, depth=depth)\n",
    "\n",
    "            _, _, done, _ = env.step(col)\n",
    "\n",
    "        if env.winner == 1:\n",
    "            wins += 1\n",
    "        elif env.winner == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return wins, losses, draws\n",
    "\n",
    "def train_q_agent(num_episodes=total_episodes, verbose=True):\n",
    "    schedule = []\n",
    "    for name, frac in curriculum:\n",
    "        count = int(frac * num_episodes)\n",
    "        schedule.extend([name] * count)\n",
    "    while len(schedule) < num_episodes:\n",
    "        schedule.append(\"selfplay\")\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    eps_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        mode = schedule[ep]\n",
    "\n",
    "        env = Connect4()\n",
    "        s_key = state_key(env.board, env.current_player)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid = env.valid_actions()\n",
    "\n",
    "            if env.current_player == 1:\n",
    "                action = choose_action(s_key, valid, epsilon)\n",
    "                board2, base_reward, done, _ = env.step(action)\n",
    "                shaped = shaped_reward(board2, -env.current_player, base_reward)\n",
    "\n",
    "                next_key = state_key(board2, env.current_player)\n",
    "                q_vals = q_table[s_key]\n",
    "                next_q = q_table[next_key]\n",
    "\n",
    "                target = shaped\n",
    "                if not done:\n",
    "                    target += gamma * np.max(next_q)\n",
    "\n",
    "                q_vals[action] += alpha * (target - q_vals[action])\n",
    "                s_key = next_key\n",
    "\n",
    "\n",
    "            else:\n",
    "                if mode == \"random\":\n",
    "                    opp = random.choice(valid)\n",
    "                elif mode == \"minimax2\":\n",
    "                    opp = minimax_agent(env, depth=2)\n",
    "                else:  # selfplay: opponent tries greedy\n",
    "                    qvals = q_table[state_key(env.board, env.current_player)]\n",
    "                    masked = np.full_like(qvals, -np.inf)\n",
    "                    valid2 = env.valid_actions()\n",
    "                    masked[valid2] = qvals[valid2]\n",
    "                    opp = int(np.argmax(masked))\n",
    "\n",
    "                _, _, done, _ = env.step(opp)\n",
    "                s_key = state_key(env.board, env.current_player)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if ep < epsilon_decay_steps:\n",
    "            epsilon = max(epsilon_end, epsilon - eps_decay)\n",
    "        else:\n",
    "            epsilon = epsilon_end\n",
    "\n",
    "        # Progress print\n",
    "        if verbose and ep % 2500 == 0 and ep > 0:\n",
    "            w, l, d = evaluate_vs_minimax(depth=2, n_games=20)\n",
    "            print(f\"Episode {ep}/{num_episodes} | eps={epsilon:.3f} | vs minimax2 (20 games): W {w} / L {l} / D {d}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return q_table\n",
    "\n",
    "\n",
    "def q_agent(env):\n",
    "    s_key = state_key(env.board, env.current_player)\n",
    "    q_vals = q_table[s_key]\n",
    "    valid = env.valid_actions()\n",
    "\n",
    "    masked = np.full_like(q_vals, -np.inf)\n",
    "    masked[valid] = q_vals[valid]\n",
    "\n",
    "    best = np.flatnonzero(masked == masked.max())\n",
    "    best = sorted(best, key=lambda c: abs(c - 3))\n",
    "    return int(best[0])\n",
    "\n",
    "\n",
    "print(\"Starting Q-agent trainingâ€¦\")\n",
    "train_q_agent()\n",
    "print(\"Q-agent ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5: game loop human vs Ai(q-learning)\n",
    "env = Connect4()\n",
    "game_over = False\n",
    "turn = 0  # 0 = human, 1 = Q-learning AI\n",
    "\n",
    "print(\"Play against Q-learning AI\")\n",
    "env.render()\n",
    "\n",
    "while not game_over:\n",
    "    if turn == 0:\n",
    "        # Human move\n",
    "        valid = env.valid_actions()\n",
    "        col = None\n",
    "        while col not in valid:\n",
    "            try:\n",
    "                col_input = int(input(f\"Your move {valid}: \"))\n",
    "                if col_input in valid:\n",
    "                    col = col_input\n",
    "            except:\n",
    "                pass\n",
    "        _, _, game_over, _ = env.step(col)\n",
    "\n",
    "    else:\n",
    "        # Q-learning AI move\n",
    "        col = q_agent(env)\n",
    "        _, _, game_over, _ = env.step(col)\n",
    "\n",
    "    env.render()\n",
    "    turn = 1 - turn\n",
    "\n",
    "    if game_over:\n",
    "        if env.winner == 1:\n",
    "            print(\"Human wins!\")\n",
    "        elif env.winner == -1:\n",
    "            print(\"AI wins!\")\n",
    "        else:\n",
    "            print(\"Draw!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 6: minimax vs q-learning\n",
    "num_simulations = 1  # number of games to simulate\n",
    "ai1_wins = 0\n",
    "ai2_wins = 0\n",
    "draws = 0\n",
    "\n",
    "# Optional: you can replace ai1_action and ai2_action with minimax_agent if you want a comparison\n",
    "for sim in range(num_simulations):\n",
    "    env = Connect4()\n",
    "    game_over = False\n",
    "    turn = 0  # 0 = Q-learning AI (ai1), 1 = Q-learning AI (ai2)\n",
    "\n",
    "    while not game_over:\n",
    "        if turn == 0:\n",
    "            col = minimax_agent(env)\n",
    "            _, _, game_over, _ = env.step(col)\n",
    "        else:\n",
    "            col = q_agent(env)\n",
    "            _, _, game_over, _ = env.step(col)\n",
    "\n",
    "        turn = 1 - turn\n",
    "\n",
    "    if env.winner == 1:\n",
    "        ai1_wins += 1\n",
    "    elif env.winner == -1:\n",
    "        ai2_wins += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "\n",
    "print(f\"After {num_simulations} games:\")\n",
    "print(f\"AI1 wins: {ai1_wins} ({ai1_wins/num_simulations*100:.1f}%)\")\n",
    "print(f\"AI2 wins: {ai2_wins} ({ai2_wins/num_simulations*100:.1f}%)\")\n",
    "print(f\"Draws   : {draws} ({draws/num_simulations*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d75ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 7: plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['AI1', 'AI2', 'Draws']\n",
    "counts = [ai1_wins, ai2_wins, draws]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(labels, counts, color=['blue','red','gray'])\n",
    "plt.title('Q-learning Self-play Results')\n",
    "plt.ylabel('Number of Games Won')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
