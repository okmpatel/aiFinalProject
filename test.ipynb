{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac60ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 episodes...\n",
      "Ep 500: Wins 108, Losses 171, Epsilon 0.803\n",
      "Weights: [-0.09   0.064  0.009 -0.14  -0.005 -0.02 ]\n",
      "Ep 1000: Wins 184, Losses 157, Epsilon 0.642\n",
      "Weights: [-0.079  0.055 -0.021 -0.168 -0.049 -0.012]\n",
      "Ep 1500: Wins 226, Losses 104, Epsilon 0.518\n",
      "Weights: [ 0.028  0.079 -0.014  0.016  0.017  0.016]\n",
      "Ep 2000: Wins 235, Losses 125, Epsilon 0.407\n",
      "Weights: [ 0.058  0.021 -0.032 -0.137 -0.019 -0.007]\n",
      "Ep 2500: Wins 257, Losses 101, Epsilon 0.322\n",
      "Weights: [ 0.041  0.09   0.016 -0.073 -0.011  0.046]\n",
      "Ep 3000: Wins 261, Losses 90, Epsilon 0.258\n",
      "Weights: [ 0.048  0.103 -0.017 -0.073 -0.032  0.009]\n",
      "\n",
      "--- READY TO PLAY ---\n",
      "\n",
      "Current Board:\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0 -1  1  1  0  0  0]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0 -1  1  1  1 -1  0]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0  0]\n",
      " [ 0  0  0 -1  1  0  0]\n",
      " [ 0 -1  1  1  1 -1  0]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0  0]\n",
      " [ 0  0  1 -1  1  0  0]\n",
      " [-1 -1  1  1  1 -1  0]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0  0]\n",
      " [ 0  1  1 -1  1  0  0]\n",
      " [-1 -1  1  1  1 -1 -1]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0 -1  0  0]\n",
      " [ 1  1  1 -1  1  0  0]\n",
      " [-1 -1  1  1  1 -1 -1]]\n",
      "AI Thinking...\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1  0  0  0 -1  0  0]\n",
      " [ 1  1  1 -1  1  0  0]\n",
      " [-1 -1  1  1  1 -1 -1]]\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1  1  0  0 -1  0  0]\n",
      " [ 1  1  1 -1  1  0  0]\n",
      " [-1 -1  1  1  1 -1 -1]]\n",
      "Human Wins!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE ENVIRONMENT\n",
    "# ==========================================\n",
    "class Connect4:\n",
    "    def __init__(self, rows=6, cols=7, connect=4):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.connect = connect\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1  # 1 = Agent, -1 = Opponent\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board.copy()\n",
    "\n",
    "    def valid_actions(self):\n",
    "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in self.valid_actions():\n",
    "            raise ValueError(f\"Invalid action {action}\")\n",
    "\n",
    "        # Drop piece\n",
    "        for r in range(self.rows - 1, -1, -1):\n",
    "            if self.board[r, action] == 0:\n",
    "                self.board[r, action] = self.current_player\n",
    "                break\n",
    "\n",
    "        # Check winner\n",
    "        winner = self.check_winner(self.board)\n",
    "        reward = 0\n",
    "        \n",
    "        if winner is not None:\n",
    "            self.done = True\n",
    "            self.winner = winner\n",
    "            # Reward: 1 if current_player won, -1 if they lost\n",
    "            reward = 1 if winner == self.current_player else -1\n",
    "        elif np.all(self.board != 0):\n",
    "            self.done = True\n",
    "            self.winner = 0\n",
    "            reward = 0 # Draw\n",
    "        \n",
    "        # Switch player if game not over\n",
    "        if not self.done:\n",
    "            self.current_player *= -1\n",
    "            \n",
    "        return self.board.copy(), reward, self.done, {}\n",
    "\n",
    "    def check_winner(self, board):\n",
    "        # Optimized check using sliding windows\n",
    "        rows, cols = board.shape\n",
    "        # Horizontal\n",
    "        for r in range(rows):\n",
    "            for c in range(cols - 3):\n",
    "                window = board[r, c:c+4]\n",
    "                if abs(sum(window)) == 4 and np.all(window != 0):\n",
    "                    return window[0]\n",
    "        # Vertical\n",
    "        for c in range(cols):\n",
    "            for r in range(rows - 3):\n",
    "                window = board[r:r+4, c]\n",
    "                if abs(sum(window)) == 4 and np.all(window != 0):\n",
    "                    return window[0]\n",
    "        # Diagonals\n",
    "        for r in range(rows - 3):\n",
    "            for c in range(cols - 3):\n",
    "                # Down-right\n",
    "                w1 = [board[r+i, c+i] for i in range(4)]\n",
    "                if abs(sum(w1)) == 4 and all(x != 0 for x in w1): return w1[0]\n",
    "                # Up-right\n",
    "                w2 = [board[r+3-i, c+i] for i in range(4)]\n",
    "                if abs(sum(w2)) == 4 and all(x != 0 for x in w2): return w2[0]\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER: MINIMAX OPPONENT (For Training)\n",
    "# ==========================================\n",
    "def evaluate_window(window, piece):\n",
    "    score = 0\n",
    "    opp_piece = -piece\n",
    "    if window.count(piece) == 4: score += 100\n",
    "    elif window.count(piece) == 3 and window.count(0) == 1: score += 5\n",
    "    elif window.count(piece) == 2 and window.count(0) == 2: score += 2\n",
    "    if window.count(opp_piece) == 3 and window.count(0) == 1: score -= 4\n",
    "    return score\n",
    "\n",
    "def score_position(board, piece):\n",
    "    score = 0\n",
    "    # Center column preference\n",
    "    center_array = [int(i) for i in list(board[:, 7//2])]\n",
    "    center_count = center_array.count(piece)\n",
    "    score += center_count * 3\n",
    "    \n",
    "    # Horizontal, Vertical, Diagonal checks...\n",
    "    # (Simplified for brevity, assumes standard connect 4 logic)\n",
    "    # Ideally, reuse the feature logic, but Minimax needs a scalar score.\n",
    "    return score # Placeholder for full heuristic\n",
    "\n",
    "def minimax(board, depth, alpha, beta, maximizingPlayer, piece):\n",
    "    valid_locations = [c for c in range(7) if board[0,c] == 0]\n",
    "    is_terminal = (Connect4().check_winner(board) is not None) or len(valid_locations) == 0\n",
    "    \n",
    "    if depth == 0 or is_terminal:\n",
    "        if is_terminal:\n",
    "            winner = Connect4().check_winner(board)\n",
    "            if winner == piece: return 100000000000000, None\n",
    "            elif winner == -piece: return -10000000000000, None\n",
    "            else: return 0, None\n",
    "        else:\n",
    "            return score_position(board, piece), None\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        value = -math.inf\n",
    "        column = random.choice(valid_locations)\n",
    "        for col in valid_locations:\n",
    "            b_copy = board.copy()\n",
    "            # Drop piece\n",
    "            for r in range(5, -1, -1):\n",
    "                if b_copy[r, col] == 0:\n",
    "                    b_copy[r, col] = piece\n",
    "                    break\n",
    "            new_score, _ = minimax(b_copy, depth-1, alpha, beta, False, piece)\n",
    "            if new_score > value:\n",
    "                value = new_score\n",
    "                column = col\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta: break\n",
    "        return value, column\n",
    "    else:\n",
    "        value = math.inf\n",
    "        column = random.choice(valid_locations)\n",
    "        for col in valid_locations:\n",
    "            b_copy = board.copy()\n",
    "            # Drop piece\n",
    "            for r in range(5, -1, -1):\n",
    "                if b_copy[r, col] == 0:\n",
    "                    b_copy[r, col] = -piece\n",
    "                    break\n",
    "            new_score, _ = minimax(b_copy, depth-1, alpha, beta, True, piece)\n",
    "            if new_score < value:\n",
    "                value = new_score\n",
    "                column = col\n",
    "            beta = min(beta, value)\n",
    "            if alpha >= beta: break\n",
    "        return value, column\n",
    "\n",
    "# ==========================================\n",
    "# 3. THE IMPROVED AGENT (Linear Approx)\n",
    "# ==========================================\n",
    "\n",
    "class LinearAgent:\n",
    "    def __init__(self, num_features=6, alpha=0.01, gamma=0.9):\n",
    "        # Weights: [Bias, My3, My2, Opp3, Opp2, Center]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.99995\n",
    "\n",
    "    def get_features(self, board, player):\n",
    "        \"\"\"\n",
    "        Extract features from the board relative to 'player'.\n",
    "        \"\"\"\n",
    "        opp = -player\n",
    "        rows, cols = board.shape\n",
    "        \n",
    "        my_3 = 0\n",
    "        my_2 = 0\n",
    "        opp_3 = 0\n",
    "        opp_2 = 0\n",
    "        \n",
    "        # Helper to analyze a window of 4 cells\n",
    "        def check(window):\n",
    "            nonlocal my_3, my_2, opp_3, opp_2\n",
    "            cnt_p = np.count_nonzero(window == player)\n",
    "            cnt_o = np.count_nonzero(window == opp)\n",
    "            cnt_e = np.count_nonzero(window == 0)\n",
    "            \n",
    "            if cnt_p == 3 and cnt_e == 1: my_3 += 1\n",
    "            elif cnt_p == 2 and cnt_e == 2: my_2 += 1\n",
    "            \n",
    "            if cnt_o == 3 and cnt_e == 1: opp_3 += 1\n",
    "            elif cnt_o == 2 and cnt_e == 2: opp_2 += 1\n",
    "\n",
    "        # Scan board (Horiz, Vert, Diag)\n",
    "        # Horizontal\n",
    "        for r in range(rows):\n",
    "            for c in range(cols - 3):\n",
    "                check(board[r, c:c+4])\n",
    "        # Vertical\n",
    "        for c in range(cols):\n",
    "            for r in range(rows - 3):\n",
    "                check(board[r:r+4, c])\n",
    "        # Diagonals\n",
    "        for r in range(rows - 3):\n",
    "            for c in range(cols - 3):\n",
    "                check([board[r+i, c+i] for i in range(4)])\n",
    "                check([board[r+3-i, c+i] for i in range(4)])\n",
    "\n",
    "        # Center Control\n",
    "        center_col = board[:, cols//2]\n",
    "        center_ctrl = np.count_nonzero(center_col == player)\n",
    "\n",
    "        # Feature Vector: [Bias, My3, My2, Opp3, Opp2, Center]\n",
    "        return np.array([1.0, my_3, my_2, opp_3, opp_2, center_ctrl])\n",
    "\n",
    "    def evaluate(self, board, player):\n",
    "        features = self.get_features(board, player)\n",
    "        return np.dot(self.weights, features)\n",
    "\n",
    "    def choose_action(self, env, training=True):\n",
    "        valid = env.valid_actions()\n",
    "        if not valid: return None\n",
    "\n",
    "        # Epsilon Greedy\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(valid)\n",
    "\n",
    "        # Greedy based on Afterstate Value\n",
    "        best_val = -float('inf')\n",
    "        best_act = random.choice(valid)\n",
    "\n",
    "        for act in valid:\n",
    "            # Simulate Next State\n",
    "            temp_board = env.board.copy()\n",
    "            for r in range(env.rows - 1, -1, -1):\n",
    "                if temp_board[r, act] == 0:\n",
    "                    temp_board[r, act] = env.current_player\n",
    "                    break\n",
    "            \n",
    "            # Check immediate win (Manual check to encourage winning)\n",
    "            if Connect4().check_winner(temp_board) == env.current_player:\n",
    "                return act\n",
    "\n",
    "            # Evaluate state VALUE\n",
    "            val = self.evaluate(temp_board, env.current_player)\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_act = act\n",
    "        \n",
    "        return best_act\n",
    "\n",
    "    def update(self, prev_board, action, reward, next_board, done, player):\n",
    "        \"\"\"\n",
    "        Gradient Descent Update:\n",
    "        Target = Reward + Gamma * V(next_best_state)\n",
    "        Error = Target - V(current_afterstate)\n",
    "        Weights += Alpha * Error * Features\n",
    "        \"\"\"\n",
    "        # 1. Feature of the state we just created (The \"Afterstate\")\n",
    "        # We need to reconstruct the board resulting from prev_board + action\n",
    "        after_board = prev_board.copy()\n",
    "        for r in range(6 - 1, -1, -1):\n",
    "            if after_board[r, action] == 0:\n",
    "                after_board[r, action] = player\n",
    "                break\n",
    "        \n",
    "        features = self.get_features(after_board, player)\n",
    "        current_val = np.dot(self.weights, features)\n",
    "\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # Bootstrap: Value of the board after opponent moves and we move again? \n",
    "            # Simplified: Value of the resulting board geometry directly\n",
    "            target = reward + self.gamma * self.evaluate(next_board, player)\n",
    "\n",
    "        error = target - current_val\n",
    "        self.weights += self.alpha * error * features\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP\n",
    "# ==========================================\n",
    "def train(episodes=5000):\n",
    "    agent = LinearAgent()\n",
    "    env = Connect4()\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    \n",
    "    print(f\"Training for {episodes} episodes...\")\n",
    "    \n",
    "    for ep in range(1, episodes+1):\n",
    "        env.reset()\n",
    "        # Randomize start order\n",
    "        agent_p = 1\n",
    "        opp_p = -1\n",
    "        \n",
    "        # If we want agent to practice going second, flip logic occasionally\n",
    "        if random.random() < 0.5:\n",
    "            # Agent is player 2 (-1), but internal logic assumes current_player\n",
    "            # For simplicity, let's keep Agent as Player 1 (1) in the env, \n",
    "            # but let Opponent move first sometimes.\n",
    "            # Opponent Move 1:\n",
    "            opp_act = random.choice(env.valid_actions())\n",
    "            env.step(opp_act)\n",
    "            \n",
    "        done = False\n",
    "        while not done:\n",
    "            # --- AGENT TURN ---\n",
    "            state_before = env.board.copy()\n",
    "            action = agent.choose_action(env)\n",
    "            \n",
    "            _, reward, done, _ = env.step(action)\n",
    "            state_after = env.board.copy()\n",
    "\n",
    "            if done:\n",
    "                # Agent Won (Reward 1) or Draw (0)\n",
    "                agent.update(state_before, action, reward, state_after, done, 1)\n",
    "                if reward == 1: wins += 1\n",
    "                break\n",
    "\n",
    "            # --- OPPONENT TURN ---\n",
    "            # Use Minimax (depth 1 or 2) for decent competition\n",
    "            # Using random occasionally helps exploration\n",
    "            if random.random() < 0.3:\n",
    "                 opp_action = random.choice(env.valid_actions())\n",
    "            else:\n",
    "                 _, opp_action = minimax(env.board, 2, -math.inf, math.inf, False, -1)\n",
    "            \n",
    "            if opp_action is None: opp_action = random.choice(env.valid_actions()) # Fallback\n",
    "            \n",
    "            _, reward_opp, done, _ = env.step(opp_action)\n",
    "            state_final = env.board.copy()\n",
    "\n",
    "            # If opponent won, reward_opp is -1 (from env perspective of who won).\n",
    "            # But relative to agent, opponent winning is a massive negative.\n",
    "            if done:\n",
    "                r = -1 if env.winner == -1 else 0\n",
    "                agent.update(state_before, action, r, state_final, done, 1)\n",
    "                if r == -1: losses += 1\n",
    "            else:\n",
    "                # Game continues. Update agent based on state AFTER opponent moved\n",
    "                agent.update(state_before, action, 0, state_final, done, 1)\n",
    "\n",
    "        if ep % 500 == 0:\n",
    "            print(f\"Ep {ep}: Wins {wins}, Losses {losses}, Epsilon {agent.epsilon:.3f}\")\n",
    "            print(f\"Weights: {np.round(agent.weights, 3)}\")\n",
    "            wins = 0; losses = 0\n",
    "\n",
    "    return agent\n",
    "\n",
    "# ==========================================\n",
    "# 5. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# 1. Train the Agent\n",
    "trained_agent = train(episodes=3000)\n",
    "\n",
    "# 2. Play a Game (Human vs AI)\n",
    "print(\"\\n--- READY TO PLAY ---\")\n",
    "env = Connect4()\n",
    "game_over = False\n",
    "env.render = lambda: print(env.board) # Simple render\n",
    "\n",
    "while not game_over:\n",
    "    print(\"\\nCurrent Board:\")\n",
    "    env.render()\n",
    "    \n",
    "    # Human Move\n",
    "    valid = env.valid_actions()\n",
    "    try:\n",
    "        col = int(input(f\"Your move (0-6) {valid}: \"))\n",
    "    except:\n",
    "        col = valid[0]\n",
    "        \n",
    "    if col in valid:\n",
    "        _, _, game_over, _ = env.step(col)\n",
    "        if game_over:\n",
    "            env.render()\n",
    "            if env.winner == 1: print(\"Human Wins!\")\n",
    "            else: print(\"Draw!\")\n",
    "            break\n",
    "            \n",
    "        # AI Move\n",
    "        print(\"AI Thinking...\")\n",
    "        # Disable epsilon for actual play\n",
    "        trained_agent.epsilon = 0 \n",
    "        col = trained_agent.choose_action(env, training=False)\n",
    "        _, _, game_over, _ = env.step(col)\n",
    "        \n",
    "        if game_over:\n",
    "            env.render()\n",
    "            if env.winner == -1: print(\"AI Wins!\")\n",
    "            else: print(\"Draw!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
